{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RecSysLoLClassifers.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZujf16p_c2y",
        "colab_type": "text"
      },
      "source": [
        "# Data Mining for Item Recommendation in MOBA Games\n",
        "Code of section 4.3 Recommender System Based on Classifiers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybR1dWGRAIda",
        "colab_type": "text"
      },
      "source": [
        "## Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACc79viArHIP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import json\n",
        "import pickle\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from sklearn.externals import joblib\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn import linear_model\n",
        "from sklearn.multiclass import OneVsRestClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGJmV8y0BwdZ",
        "colab_type": "text"
      },
      "source": [
        "## Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D81V32smDEc8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "# Authenticate and create the PyDrive client.\n",
        "# This only needs to be done once per notebook.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9xIbKHrJmyA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def open_pickle(path):\n",
        "    pickle_in = open(path,\"rb\")\n",
        "    example_dict = pickle.load(pickle_in)\n",
        "    df=example_dict[0]\n",
        "    pickle_in.close()\n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLMFGSqMEtyB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# file ids of datasets\n",
        "file_id = ['1QyC2zho409ctBVSEgylOsqSGdfzPi7On',\n",
        "          '1-4Pqyd1ivq-o33UdzuH4eIgjoq-QdTy5',\n",
        "          '1GBWilejUnIatT7o0QvRUwMefo-ydPAjl',\n",
        "          '1ipgQtMTc_Z-LJuQAtYHTi75jP3UQ1AT5',\n",
        "          '139b8gxLRa02x29p13eNDrGlFC-zMG8hM']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNkNC0zVD5cL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for id in file_id:\n",
        "  fileId = drive.CreateFile({'id': id})\n",
        "  print(fileId['title'])\n",
        "  fileId.GetContentFile(fileId['title']) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oa9L6yMNHZXF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load training set\n",
        "print('Loading training set..')\n",
        "df = open_pickle('baseline_train_labels_splits.pkl')\n",
        "Y_train = df.values\n",
        "\n",
        "df = open_pickle('baseline_train_inputs_splits.pkl')\n",
        "X_train = df.values\n",
        "\n",
        "# load test set\n",
        "print('Loading test set..')\n",
        "df = open_pickle('baseline_test_labels_splits.pkl')\n",
        "Y_test = df.values\n",
        "item_ids = df.columns.tolist()\n",
        "\n",
        "df = open_pickle('baseline_test_inputs_splits.pkl')\n",
        "X_test = df.values\n",
        "\n",
        "# load items data\n",
        "print('Loading items information..')\n",
        "with open('items.json') as items_json:\n",
        "    data_items = json.load(items_json)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yROjzesAuYw",
        "colab_type": "text"
      },
      "source": [
        "## Utils"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lk6BclRHIhVo",
        "colab_type": "text"
      },
      "source": [
        "### Transformations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aaxJrxvVrkiL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "id_dictionary = {}\n",
        "for i, j in enumerate(item_ids):\n",
        "    id_dictionary.update({i:j}) # create a indexed dictionary from the items's ids\n",
        "\n",
        "## create function that recieves a one-hot list and returns the respective ids\n",
        "def translate_onehot(onehot, name=True):\n",
        "    ids = [] # create an empty list to collect the ids of the items\n",
        "    names = [] # create an empty list to collect the names of the items\n",
        "    idxs = [i for i, e in enumerate(onehot) if e == 1] # extract indexes of encoded items in the one-hot vector\n",
        "    for idx in idxs: # iterate over the list of indexes\n",
        "        id = id_dictionary[idx] # search the correspondent id according to the index\n",
        "        ids.append(id) # append the id to the list of ids\n",
        "        id_dict = (next(item for item in data_items if item['id'] == str(id))) # search the dictionary that corresponds to the item's id\n",
        "        names.append(id_dict['name']) # extract the name of the item from this dictionary\n",
        "    if name:\n",
        "        return ids, names\n",
        "    else:\n",
        "        return ids\n",
        "\n",
        "## create a function that recieves a output of a classifier and returns a @k itemset\n",
        "def decoding(prediction, itemset_size, id_items):\n",
        "    index_max = np.argsort(prediction)\n",
        "    _set = index_max[(len(id_items)-itemset_size):len(id_items)]\n",
        "    items = np.asarray(id_items)\n",
        "    itemset = items[np.flip(_set)]\n",
        "    return itemset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8decmbjIzvs",
        "colab_type": "text"
      },
      "source": [
        "### Metrics\n",
        "Forked from https://gist.github.com/bwhite/3726239"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1-2C7ye45D7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def precision_at_k(r, k):\n",
        "    \"\"\"Score is precision @ k\n",
        "\n",
        "    Relevance is binary (nonzero is relevant).\n",
        "\n",
        "    >>> r = [0, 0, 1]\n",
        "    >>> precision_at_k(r, 1)\n",
        "    0.0\n",
        "    >>> precision_at_k(r, 2)\n",
        "    0.0\n",
        "    >>> precision_at_k(r, 3)\n",
        "    0.33333333333333331\n",
        "    >>> precision_at_k(r, 4)\n",
        "    Traceback (most recent call last):\n",
        "        File \"<stdin>\", line 1, in ?\n",
        "    ValueError: Relevance score length < k\n",
        "\n",
        "\n",
        "    Args:\n",
        "        r: Relevance scores (list or numpy) in rank order\n",
        "            (first element is the first item)\n",
        "\n",
        "    Returns:\n",
        "        Precision @ k\n",
        "\n",
        "    Raises:\n",
        "        ValueError: len(r) must be >= k\n",
        "    \"\"\"\n",
        "    assert k >= 1\n",
        "    r = np.asarray(r)[:k] != 0\n",
        "    if r.size != k:\n",
        "        raise ValueError('Relevance score length < k')\n",
        "    return np.mean(r)\n",
        "\n",
        "def average_precision(r):\n",
        "    \"\"\"Score is average precision (area under PR curve)\n",
        "\n",
        "    Relevance is binary (nonzero is relevant).\n",
        "\n",
        "    >>> r = [1, 1, 0, 1, 0, 1, 0, 0, 0, 1]\n",
        "    >>> delta_r = 1. / sum(r)\n",
        "    >>> sum([sum(r[:x + 1]) / (x + 1.) * delta_r for x, y in enumerate(r) if y])\n",
        "    0.7833333333333333\n",
        "    >>> average_precision(r)\n",
        "    0.78333333333333333\n",
        "\n",
        "    Args:\n",
        "        r: Relevance scores (list or numpy) in rank order\n",
        "            (first element is the first item)\n",
        "\n",
        "    Returns:\n",
        "        Average precision\n",
        "    \"\"\"\n",
        "    r = np.asarray(r) != 0\n",
        "    out = [precision_at_k(r, k + 1) for k in range(r.size) if r[k]]\n",
        "    if not out:\n",
        "        return 0.\n",
        "    return np.mean(out)\n",
        "\n",
        "def mean_average_precision(rs):\n",
        "    \"\"\"Score is mean average precision\n",
        "\n",
        "    Relevance is binary (nonzero is relevant).\n",
        "\n",
        "    >>> rs = [[1, 1, 0, 1, 0, 1, 0, 0, 0, 1]]\n",
        "    >>> mean_average_precision(rs)\n",
        "    0.78333333333333333\n",
        "    >>> rs = [[1, 1, 0, 1, 0, 1, 0, 0, 0, 1], [0]]\n",
        "    >>> mean_average_precision(rs)\n",
        "    0.39166666666666666\n",
        "\n",
        "    Args:\n",
        "        rs: Iterator of relevance scores (list or numpy) in rank order\n",
        "            (first element is the first item)\n",
        "\n",
        "    Returns:\n",
        "        Mean average precision\n",
        "    \"\"\"\n",
        "    return np.mean([average_precision(r) for r in rs])\n",
        "\n",
        "def mean_reciprocal_rank(rs):\n",
        "    \"\"\"Score is reciprocal of the rank of the first relevant item\n",
        "\n",
        "    First element is 'rank 1'.  Relevance is binary (nonzero is relevant).\n",
        "\n",
        "    Example from http://en.wikipedia.org/wiki/Mean_reciprocal_rank\n",
        "    >>> rs = [[0, 0, 1], [0, 1, 0], [1, 0, 0]]\n",
        "    >>> mean_reciprocal_rank(rs)\n",
        "    0.61111111111111105\n",
        "    >>> rs = np.array([[0, 0, 0], [0, 1, 0], [1, 0, 0]])\n",
        "    >>> mean_reciprocal_rank(rs)\n",
        "    0.5\n",
        "    >>> rs = [[0, 0, 0, 1], [1, 0, 0], [1, 0, 0]]\n",
        "    >>> mean_reciprocal_rank(rs)\n",
        "    0.75\n",
        "\n",
        "    Args:\n",
        "        rs: Iterator of relevance scores (list or numpy) in rank order\n",
        "            (first element is the first item)\n",
        "\n",
        "    Returns:\n",
        "        Mean reciprocal rank\n",
        "    \"\"\"\n",
        "    rs = (np.asarray(r).nonzero()[0] for r in rs)\n",
        "    return np.mean([1. / (r[0] + 1) if r.size else 0. for r in rs])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLu7VLukI8hR",
        "colab_type": "text"
      },
      "source": [
        "## ANN RecSys Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYdk2BLeJTN5",
        "colab_type": "text"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y1_eEn8uHHvD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def trainANN(X_train, Y_train, input_size, target_size, save=True):\n",
        "    \"\"\" Function to train a neural net of 2 layer\n",
        " \n",
        "        Parameters\n",
        "        ----------\n",
        "        X_train: inputs\n",
        "        Y_train: targets\n",
        "        input_size: input size of the network\n",
        "        target_size: ouput size of the network\n",
        "        save: if True, save model\n",
        "    \"\"\"\n",
        "    # define modelo y lo entrena\n",
        "    model = Sequential()\n",
        "    model.add(Dense(150, input_dim=input_size, activation='relu'))\n",
        "    model.add(Dense(150, activation='relu'))\n",
        "    model.add(Dense(target_size, activation='sigmoid'))\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    model.fit(X_train, Y_train, epochs=1, batch_size=128, verbose=1, validation_split=0.1, shuffle=True)\n",
        "    if save:\n",
        "        # serializar modelo a JSON\n",
        "        model_json = model.to_json()\n",
        "        with open(\"model.json\", \"w\") as json_file:\n",
        "            json_file.write(model_json)\n",
        "        # serializar pesos a HDF5\n",
        "        model.save_weights(\"model.h5\")\n",
        "        print(\"Model saved\")\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZz4dBsIrz8U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train and save the model\n",
        "input_size = 136 # number of champions\n",
        "target_size = 89 # number of items\n",
        "model = trainANN(X_train, Y_train, input_size, target_size, save=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8yKXAKvJxlv",
        "colab_type": "text"
      },
      "source": [
        "### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZXHdZlD4MDL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# predictions of test set\n",
        "Y_p = model.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghKRz6LQ4843",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "eval_itemset_size = 6 # size of itemset required\n",
        "\n",
        "Relevance=[]\n",
        "Precision = []\n",
        "Recall = []\n",
        "F1 = []\n",
        "\n",
        "for i in tqdm(range(X_test.shape[0]-1)):\n",
        "    targ_items=translate_onehot(Y_test[i], name=False)\n",
        "    # decode output of the model\n",
        "    itemset = decoding(Y_p[i],itemset_size=eval_itemset_size,id_items=item_ids)\n",
        "    rel_items = [int(j in targ_items) for j in itemset]\n",
        "    Relevance.append(rel_items)\n",
        "    num = list(set(itemset) & set(targ_items))\n",
        "    if len(targ_items) >= 1:\n",
        "        precision = (len(num) / len(itemset))\n",
        "        recall = (len(num) / len(targ_items))\n",
        "        Precision.append(precision)\n",
        "        Recall.append(recall)\n",
        "        if precision == 0.0 and recall == 0.0:\n",
        "            F1.append(0)\n",
        "        else:\n",
        "            F1.append(2 * ((precision * recall) / (precision + recall)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWqOUjUM5HFq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "_map=mean_average_precision(Relevance)\n",
        "mrr=mean_reciprocal_rank(Relevance)\n",
        "\n",
        "print('MAP:',_map)\n",
        "print('MRR:',mrr)\n",
        "print('Precision:', np.mean(Precision))\n",
        "print('Recall:', np.mean(Recall))\n",
        "print('F1:', np.mean(F1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wu-pzA1xMM5v",
        "colab_type": "text"
      },
      "source": [
        "## Logit or Decision Tree RecSys Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-9cp5Z44P-q",
        "colab_type": "text"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6i0xXAV1M6wy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(X_tr, Y_tr, model_type, save=True):\n",
        "    if model_type == 'logit':\n",
        "      clf = OneVsRestClassifier(linear_model.SGDClassifier(max_iter=1000, tol=1e-3, loss='log', verbose=True))\n",
        "    elif model_type == 'dtree':\n",
        "      clf = OneVsRestClassifier(DecisionTreeClassifier(random_state=0))\n",
        "    clf.fit(X_tr, Y_tr)\n",
        "    print('Training finished')\n",
        "    if save:\n",
        "        # save model\n",
        "        joblib.dump(clf, 'model.pkl')\n",
        "        print('Model saved')\n",
        "    return clf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ttpT5OoTNvNJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train and save the model\n",
        "model_type = 'logit' # choose 'logit' or 'dtree'\n",
        "clf = train(X_train, Y_train, model_type, save=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1ozDlhmNokp",
        "colab_type": "text"
      },
      "source": [
        "### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1y-OLhx5Nox6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# predictions of test set\n",
        "Y_p = clf.predict_proba(X_test)\n",
        "pred_classes=clf.classes_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tDzz_VfxPd6n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "eval_itemset_size = 6 # size of itemset required\n",
        "target_size = 89 # number of items\n",
        "\n",
        "Relevance=[]\n",
        "Precision = []\n",
        "Recall = []\n",
        "F1 = []\n",
        "\n",
        "for i in tqdm(range(X_test.shape[0]-1)):\n",
        "    targ_items=translate_onehot(Y_test[i], name=False)\n",
        "    # adjusting list size\n",
        "    real_pred = []\n",
        "    aux = 0\n",
        "    for j in range(target_size):\n",
        "        if j == pred_classes[aux]:\n",
        "            real_pred.append(Y_p[i][aux])\n",
        "            aux += 1\n",
        "        else:\n",
        "            real_pred.append(0)\n",
        "    # decode output of the model\n",
        "    itemset = decoding(real_pred,itemset_size=eval_itemset_size,id_items=item_ids) #itemset_size = @\n",
        "    rel_items = [int(j in targ_items) for j in itemset]\n",
        "    Relevance.append(rel_items)\n",
        "    num = list(set(itemset) & set(targ_items))\n",
        "    if len(targ_items) >= 1:\n",
        "        precision = (len(num) / len(itemset))\n",
        "        recall = (len(num) / len(targ_items))\n",
        "        Precision.append(precision)\n",
        "        Recall.append(recall)\n",
        "        if precision == 0.0 and recall == 0.0:\n",
        "            F1.append(0)\n",
        "        else:\n",
        "            F1.append(2 * ((precision * recall) / (precision + recall)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Enf2neXR6s4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "_map=mean_average_precision(Relevance)\n",
        "mrr=mean_reciprocal_rank(Relevance)\n",
        "\n",
        "print('MAP:',_map)\n",
        "print('MRR:',mrr)\n",
        "print('Precision:', np.mean(Precision))\n",
        "print('Recall:', np.mean(Recall))\n",
        "print('F1:', np.mean(F1))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}